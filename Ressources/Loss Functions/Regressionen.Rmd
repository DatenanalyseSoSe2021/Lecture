---
title: "Regressions"
author: "Yilun"
date: "5/17/2021"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(ISLR)
data(Auto)
library(ggplot2)
library(dplyr)

```

Let us take a quick look at the distribution of weights and compute summary statistics:

```{r}
hist(Auto$weight, xlab = "weight", col = "blue");grid()
(weightSum = summary(Auto$weight))

abline(v=weightSum["Median"],col=3)
abline(v=weightSum["Mean"],col=2)
abline(v=weightSum["3rd Qu."],col="orange", lty=2)
```


We can see a marked difference between the mean and the median (why again?). 
That brings up the general question, how to choose from the various "location measures" of a distribution, such as the mean, median, trimmed mean, geometric mean, harmonic mean, ...

## Loss Functions

Can we define an objective optimality measure which would clearly favor one metric over another ? 
Welcome to the concept of a **loss function**.
We all feel intuitively that the orange line (at 3615 lb) is in some sense inferior to the red and green numbers as a location measure.

Why? Because the distance from the data to orange is (on the average) larger than the distance to red and green. Let us call that **average distance a "loss"** and assume that we want to **minimize loss**.

It turns out that there is not just one but various ways to define this distance/loss function ("LE" = location estimate):

1. Minimized by the **Modus**  $$ L_0 = (1/n) \cdot \sum_{i=1}^n{|x_i - LE|^0}$$
2. Minimized by the **Median**  $$ L_1 = (1/n) \cdot \sum_{i=1}^n{|x_i - LE|^1}$$
3. Minimized by the **Mean** $$ L_2 = (1/n) \cdot \sum_{i=1}^n{|x_i - LE|^2}$$

## SLR

Simple Linear Regression (SLR) 

Population Version:
$$
y_i = \beta_0 + \beta_1 \cdot x_i + \epsilon_i
$$
Sample Version (Estimated from data):
$$
y_i = \hat{\beta}_0 + \hat{\beta}_1 \cdot x_i + \hat{\epsilon}_i = \hat{y}_i + \hat{\epsilon}_i
$$

Loss Function = **residual sum of squares** (RSS) !

Minimizing $RSS = \sum_{i=1}^n{\hat{\epsilon}_i^2}$ leads to the following **Least Squares** coefficient estimates:

\begin{align}
\hat{\beta}_1 & = \frac{\sum_{i=1}^{n}{(x_i - \bar{x})(y_i - \bar{y})}}{\sum_{i=1}^{n}{(x_i - \bar{x})^2}} \label{eq:betaHat1} \\
\hat{\beta}_0 & = \bar{y} - \hat{\beta_1} \bar{x}
\end{align}

The first equation is simply the sample covariance between $x$ and $y$ divided by the sample variance of x: $\hat{\beta}_1 = cov_{xy}/var_x$. (why?)


## Regression Line

```{r}

plot(mpg ~ weight, data = Auto,col = rgb(0,0,1,0.5), pch=20,xlim=c(250, 7000), ylim = c(0,45));grid()
LSfit = lm(mpg ~ weight, data = Auto)

#overlay regression line
abline(LSfit, col="darkgreen",lwd=2)

#summary
summary(LSfit)

#diagnostics
#plot(LSfit, c(1,2,5))

```


* Use the correlation coefficient to compute the slope.
* Interpret the slope, precisely.

If we increase the weight by one unit the average mpg decreases by 0.007 units.

```{r, echo=FALSE}
plot(c(1,2),c(3,5), xlab="",ylab="",pch=19, cex=1.5, xlim = c(0,3), ylim = c(1,6), col ="blue");grid()
lines(c(1,2),c(3,3), col = "green", lwd=2)
lines(c(2,2),c(3,5), col = "green", lwd=2)
lines(c(0,2.5),c(1,6), col = "red", lwd=1.5)
text(1,2.6,expression(paste("(", x[1], ",", y[1], ")")), cex=1.5, col ="blue")
text(2,4.6,expression(paste("(", x[2], ",", y[2], ")")), cex=1.5, col ="blue")
```

$$
\beta_1 = \frac{\Delta y}{\Delta x} =  \frac{y_2-y_1}{x_2-x_1}
$$




## Exercise 17 May

### Residuals

* Verify that the sum of the residuals is zero
* Verify the **Least Squares Property** of the regression line.

```{r}
# Verify sum of residual is zero
Auto %>%
  summarise(residual = (LSfit$coefficients[2]* weight + LSfit$coefficients[1])- mpg)%>%
  sum()

```


### $R^2$

* Compute $R^2$ using two different expressions:
  * $R^2 = r^2$ 
  * $R^2 = 1 - RSS/TSS$ 

```{r}
#Method 1 
cor(Auto$weight,Auto$mpg)^2

#Method 2
RSS = sum(LSfit$residuals^2)
TSS = sum((Auto$mpg-mean(Auto$mpg))^2)

1-(RSS/TSS)
```

